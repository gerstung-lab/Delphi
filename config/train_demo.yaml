eval_interval: 250 # keep frequent because we'll overfit
eval_iters: 25
eval_only: false
init_from: scratch
seed: 42
gradient_accumulation_steps: 1
batch_size: 256
device: mps

train_data:
  data_dir: ukb_simulated_data
  memmap_fname: train.bin
  tokenizer_fname: tokenizer.yaml
  transforms:
    - name: no-event
      args:
        interval_in_years: 5
        mode: random
val_data:
  data_dir: ukb_simulated_data
  memmap_fname: val.bin
  tokenizer_fname: tokenizer.yaml
  transforms:
    - name: no-event
      args:
        interval_in_years: 5
        mode: random
model:
  vocab_size: 1271
  n_layer: 12
  n_head: 12
  n_embd: 120
  dropout: 0.1
  token_dropout: 0.0
  t_min: 0.1
  bias: true
  mask_ties: true
  ignore_tokens: [0, 2 ,3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # ignore padding and lifestyle tokens
  loss:
    ce_beta: 1.0
    dt_beta: 1.0
    zero_inflate: false
log:
  wandb_log: true
  wandb_project: delphi_debug
  run_name: delphi_demo
  log_interval: 25
  always_ckpt_after_eval: true
  ckpt_interval: 1000
optim:
  learning_rate: 2e-3 # with baby networks can afford to go a bit higher
  max_iters: 5000
  weight_decay: 2e-1
  lr_decay_iters: 5000 # make equal to max_iters usually
  min_lr: 2e-4 # learning_rate / 10 usually
  beta2: 0.99 # make a bit bigger because number of tokens per iter is small
  warmup_iters: 500 # not super necessary potentially
