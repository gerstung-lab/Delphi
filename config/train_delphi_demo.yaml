ckpt_dir: ${oc.env:DELPHI_CKPT_DIR}
eval_interval: 250 # keep frequent because we'll overfit
eval_iters: 25
log_interval: 25 # don't print too too often
eval_only: false
# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint: false
init_from: scratch
seed: 42

wandb_log: true # override via command line if you like
wandb_project: delphi
wandb_run_name: delphi_demo

data_dir: ${oc.env:DELPHI_DATA_DIR}
dataset: ukb_simulated_data
gradient_accumulation_steps: 1
batch_size: 256

model:
  block_size: 48
  vocab_size: 1271
  n_layer: 12
  n_head: 12
  n_embd: 120
  dropout: 0.1
  token_dropout: 0.0
  t_min: 0.1
  bias: true
  mask_ties: true
  ignore_tokens: [0, 2 ,3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # ignore padding and lifestyle tokens
  zero_time_inflation: false

learning_rate: 2e-3 # with baby networks can afford to go a bit higher
max_iters: 5000
weight_decay: 2e-1
lr_decay_iters: 5000 # make equal to max_iters usually
min_lr: 2e-4 # learning_rate / 10 usually
beta2: 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters: 500 # not super necessary potentially

device: cpu

data_fraction: 1.0
no_event_token_rate: 5
